

\documentclass[12pt,a4paper]{article}


%Spacing Packages
\usepackage{fullpage}
\usepackage{a4wide}

%Other Packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{euscript}

\usepackage[lf]{venturis} %% lf option gives lining figures as default; 
			  %% remove option to get oldstyle figures as default
\usepackage[T1]{fontenc}

\begin{document}

\begin{quote}

\begin{center} {\large 24.118x -- Paradox and Infinity \\ \vspace{1mm}}
 {\large Problem Set 2: Answer sheet \\ \vspace{1mm}}
 
\end{center}
\vspace{3mm}



\end{quote} 

\subsection*{Problems:}

\begin{enumerate}
  
  \item According to an evidential decision theorist, you should burn the thousand. Given the observed correlation between what happens on this side of the plane and what happens on the other side, it is overwhelmingly likely that, if you don't burn the thousand, neither will your twin, and you'll end up with a thousand dollars. On the other hand, it is overwhelmingly likely that, if you burn the thousand, your twin will too, and you'll end up with a million dollars. So you get good evidence you will end up with a million dollars rather than a thousand dollars by burning the money. So, the evidential decision theorist says: burn!
  
  \item According to a causal decision theorist, you shouldn't burn the thousand. When the plane is opaque, there is no causal influence between this side of the plane and the other side. So your twin is either going to burn her money, or not; there's nothing you can do to affect what she does. Now, if she were to not burn her money, you would obviously be better off not burning your money; otherwise, you would end up with nothing. And suppose she were to burn her money? Then you would still be better off not burning your money; you would end up with a million plus a thousand, rather than just a million. Either way, you would be better off not burning the money. So: don't burn the money!
 
 \item Most scenarios are such that if the indicative conditional is true, so is the corresponding counterfactual conditional. Suppose I am holding some chalk, as I am prone to do. The following is true:
 \begin{quote}
If I drop this chalk, it will fall to the ground.
\end{quote}

Later, \emph{this} is true:
\begin{quote}
If I had dropped the chalk, it would have fallen to the ground.
\end{quote}

\item A scenario in which the indicative conditional is true and the corresponding counterfactual is not is a scenario in which there is evidential dependence between two events but no causal dependence. A good way of coming up with scenarios like that is to think of scenarios involving events with a common cause, but with no direct causal connection. For example, the state of the atmosphere now can cause rain later today, and it can cause meteorologists to predict rain, which causes them to update websites accordingly, which causes come people to carry umbrellas.

Perhaps you have a friend Lakshmi who you know to always check the weather websites in the morning, and to always carry an umbrella on those days on which rain is predicted. You see her each morning on the bus, and you know that

\begin{quote}
If Lakshmi brings her umbrella, it's going to rain.
\end{quote}

Now suppose that one day she's not carrying an umbrella, and it doesn't rain. Is the following true?

\begin{quote}
If Lakshmi had brought her umbrella, it would have rained.
\end{quote}

No! 

\item Here's a tickle defence for the Newcomb case. 

The reason that you, the two-boxer, are having the intuition you are having about the Newcomb case is that you have in mind a different, but similar, case -- a case where you have more information than you are explicitly given in the Newcomb case. 

When you are presented with the two boxes, you have an initial inclination to either one-box or two-box. You know which inclination you have. The extra information you are assuming is that predictor's accuracy is the result of a correlation between her prediction and the subject's initial inclination, and only via that is there a correlation with the final decision of Newcomb subjects.

If that's right, then the evidence you get about what's in the opaque box, when you discover your initial inclination, \emph{screens off} the evidence you get from your final decision. Evidential decision theory says that, in that case, once you've learned about your initial inclination, you should two-box, just like your (two-boxing) intuition says.

\item You could give either answer, as long as you could provide a decent reason for your answer. The standard answer in game theory is that you should defect, and the standard reasoning is dominance reasoning: you are better off defecting if Jones keeps quiet, and you are better off defecting if Jones defects.

\item Again, you could give either a 'yes' or a 'no' answer to this question, as long as you had a good reason, and it cohered with what you said in the previous question. You needed to give evidential-decision-theory kinds of reasons to support the conclusion that, in this situation, you should keep quiet, and you needed to give causal-decision-theory kinds of reasons to support the conclusion that, in this situation, you should defect. And what you said needed to be compatible with the reasons you gave in the previous question.

\end{enumerate}



\end{document}

