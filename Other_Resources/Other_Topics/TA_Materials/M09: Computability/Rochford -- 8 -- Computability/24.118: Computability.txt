Title: 24.118: Computability
Author: Damien Rochford (based on notes by Agustín Rayo)

* [Turing Machines][section_machines]
	* [Hardware][section_hardware]
	* [Software][section_software]
	* [Running and Halting][section_running]
	* [An Example][section_example]
	* [A Simlulator][section_simulator]
* [Computability][section_computability]
	* [Computing a Function][section_function]
	* [The Church-Turing Thesis][section_church-turing]
* [Numbering Turing Machines][section_numbering]
	* [Two-Symbol Machines][section_two-symbol]
	* [Our Scheme][section_scheme]
* [Non-Computable Functions][section_non-computable]
	* [The Halting Function][section_halting]
		* [The Proof][section_proofhalting]
	* [The Busy Beaver Function][section_busy]
		* [The Proof][section_proofbusy]
	* [The Universal Turing Machine][section_universal]
* [P=NP?][section_p=np?]
	* [Tractability][section_tractability]
	* [Measuring Complexity][section_measuring]
	* [Exponential Time][section_exponential]
	* [Polynomial Time][section_polynomial]
	* [P vs. NP][section_pvsnp]


In this topic we’ll be talking about *computability*. Computability is a property of functions — specifically, of functions from natural numbers to natural numbers. Some of them are computable, and some of them aren’t. That sounds a bit abstract, but it has very practical consequences. Every program that any actual computer runs can be thought of as a function. So the fact that some functions are incomputable means that there are some programs that *no* computer could run — not a pocket calculator (does anyone have those any more?) not your laptop, not 天河-2 (the Tianhe-2), which can perform 33.86 quadrillion operations per second, and not any computer anyone could even in principle build.

If you want to know about computability, the first thing you need to know about is Turing Machines.

#Turing Machines [section_machines]

[Alan Turing][2] was a British mathematician and perhaps the most important most responsible for the existence of computer science. He was the first to describe Turing Machines, which are certain kind of idealised model of a computer.

Turing Machines are particularly simple. Let me describe them for you.

[2]:https://en.wikipedia.org/wiki/Alan_Turing

##Hardware [section_hardware]

A Turing Machine’s hardware consists of two components: 

1. A memory tape

You can think of the tape as a long strip of paper:

A long strip of paper, divided into cells.
  
The strip has been divided into cells; each cell can hold, at most, one symbol in it. We may assume that our strip of paper is, in effect, infinite in both directions, because an assistant is ready to add paper to either end, if needed.

2. A tape-reader

At any given time, the reader sits on a particular cell of the memory tape. We will indicate the position of the reader by placing arrow beneath the cell at which the reader is positioned:

 The same strip as above, with a particular cell singled out with an arrow below it.

The reader is able to perform any of the following functions: 

* read the symbol which is written on the reader’s current location;
* replace that symbol with a new one;
* move one cell to the left;
* move one cell to the right.

The reader is also able to be in a variety of internal states. How many states depends on the Turing Machine, but it always is some finite amount.

That’s it! That’s all the hardware we’re dealing with.

##Software [section_software]

What about software? Each Turing Machine has exactly one “program”. A Turing Machine program consists of a finite list of “command lines”. Each command line is a sequence of the following form:

<current state> <current symbol> <new symbol> <direction> <new state>

and is interpreted as the following instruction:

If you are in <current state> and your reader sees <current symbol> written on the memory tape, replace <current symbol> with <new symbol>. Then move one step <direction>, and go to <new state>.

The <current state> and <new state> parameters should be filled with numerals (‘0’, ‘1’, ‘2’, etc.), and the <direction> parameter should be filled with ‘l’ (for ‘left), ‘r’ (for ‘right’), or  ‘*’ (for ‘don’t move’). For instance,

7 0 1 r 2

means:

If you are in state 7 and your reader sees “0” written on the memory tape, replace the “0” with a “1”. Then move one step to the right, and go to state 2.

##Running and Halting [section_running]

A Turing Machine always starts out in state 0, and runs in steps. At each step, the machine performs the instruction corresponding to the command line that matches its current state and the symbol which it is currently reading. (Or the first such command line, if there is more than one.) Then it goes on to the next step.

If at any stage the machine is unable to find a command line that matches its present state and the symbol which it is reading, the machine halts. If not, it keeps running.

(To make Turing Machine programs easier for humans to understand, programmers sometimes use “halt” as a name for a non-existing state.)

##An Example [section_example]

Consider the Turing machine whose program consists of the following two commands:

0 _ k * 1

0 a o r 0

and suppose that the tape starts out as follows:

The same strip and arrow as above, except the arrow has a '0' subscript, and the cell the arrow is pointing to contains a letter 'a'.
(The numeral next to the arrow indicates the current state of the machine. I have written down a ‘0’ because Turing machines always start out in state 0.)

This is what will happen:

Step 1
When the machine is switched on, it will be in state 0, and the reader will be reading symbol “a”. So the machine will follow the instructions in command line: “0 a o r 0”. In other words, it will replace the “a” with an “o”, move one cell to the right, and remain in state 0:

A long strip divided into cells. In one cell, there is a letter 'o'. An arrow points to the cell immediately to the right. The arrow as a '0' subscript.

Step 2
The machine is still in state 0 but now it is reading a blank. So it follows the instructions in command line: “0 _ k * 1”. In other words, it replaces the blank with a ‘k’, remains in its current position, and switches to state 1:

Same strip as above, except the cell to which the arrow is pointing contains a 'k'. The arrow is subscripted with a '1'.

Step 3
The machine is now in state 1, reading a ‘k’. But it has no command line that starts with “1 k”. So the machine halts.

##A Simulator [section_simulator]

You can find a Turing Machine simulator [here][1] (it was written by Anthony Morphett, a mathematician at the University of Melbourne). The best way to learn about Turing Machines is to play with them. Start learning by using the simulator to complete the following exercises:

1. Suppose the machine starts out with a blank tape. Design a Turing Machine that writes a ‘1’ on two consecutive cells, and halts.
2. Suppose the tape starts out with a finite sequence of zeroes and ones (followed by blanks), and that the reader is positioned at the first member of the sequence. Design a machine that replaces the zeroes with ones and the ones with zeroes, and halts.
3. Suppose the tape starts out with a finite sequence of ones, and that the reader is positioned at the first member of the sequence. Design a machine that doubles the length of the string of ones, and halts.
4. Suppose the tape starts out with a sequence of n ones, and that the reader is positioned at the first member of the sequence. Design a machine that replaces the sequence of n ones with a sequence of zeroes and ones that corresponds to n in binary notation.

[1]:http://morphett.info/turing/turing.html

#Computability [section_computability]

##Computing A Function [section_function]

Suppose \(f\) is a function from natural numbers to natural numbers. Say that an ordinary computer (running an ordinary computer program) computes \(f\) if and only if the following holds for each natural number \(n\):

>If you give the ordinary computer n as input, you’ll get \(f(n)\) as output.

We can define a version of this idea for Turing Machines. We can think of a Turing machine as taking number \(n\) as input if it starts out with a tape that is completely blank except for a sequence of \(n\) ones (with the reader positioned at the left-most one). And we can think of it as delivering number \(f(n)\) as output if it halts with a tape that is completely blank except for a sequence of \(f(n)\) ones (with the reader positioned at the left-most one).

Accordingly, a Turing Machine **computes function \(f\)** if and only if the following holds for each natural number \(n\):

>If the Turing Machine starts out with a tape that is completely blank except for a sequence of \(n\) ones (with the reader positioned at the left-most one), the machine will halt with a tape that is completely blank except for a sequence of \(f(n)\) ones (with the reader positioned at the left-most one).

We can extend this definition to functions that take more than one number as input. So, for instance a Turing Machine computes a two-place function \(f\) iff 
>When the tape starts out with a sequence of \(n\) ones, followed by a blank, followed by a sequence of \(m\) ones (and when the reader starts out at the beginning of all that), the Turing Machine halts with a tape that is completely blank except for a sequence of \(f(n,m)\) ones (with the reader positioned at the left-most one).

Now, each Turing Machine can be identified with its program, which is some finite list of command lines, each of which is a tuple of five symbols. There are countably many such programs, so there are countably many Turing Machines.

On the other hand, there are uncountably many functions from the natural numbers to the natural numbers, as you proved back in Topic 3.

So we know there’s got to be functions from the natural numbers to the natural numbers that are not computable by any Turing Machine — there aren’t enough Turing Machines to compute them all.

##The Church-Turing Thesis [section_church-turing]

You might think: who cares if a function isn’t Turing computable? Turing Machines are so simple; you wouldn’t expect them to be able to do very much. There are lots of ways of making a Turing machine more fancy, and you might have the hope that one of these fancier machines can compute functions that a Turing machine can’t. Instead of a linear tape, you might use a two-dimensional grid. Instead of a single tape-reader, you might use many readers. You might use many tapes, working in parallel. You might give each cell on the tape an address, and the reader the ability to go to an address without going through other cells first (i.e., you might use a machine with RAM).

Here is an amazing fact. No more fancy machine that anyone has ever thought of can do better than a plain old Turing Machine. That is: nobody has ever figured out a way of computing functions that can compute functions that a Turing machine cannot compute.

This fact is some evidence for the following thesis, which most computer scientists believe:

>**The Church-Turing Thesis**
> A function is computable in any way whatsoever iff it is computable by a Turing machine.
That is: Turing computability just is computability.

(The “Church” of the “Church-Turing Thesis” is [Alonzo Church][3], an American logician, who came up with his own way of characterising computability, called the “[lambda calculus][4]”, which turned out to be equivalent to the Turing-Machine way. The lambda calculus is great in its own right; in some ways it is a more beautiful way of understanding computability than the Turing-Machine way, though its relationship to physical computers is less obvious.)

[3]:https://en.wikipedia.org/wiki/Alonzo_Church
[4]:https://en.wikipedia.org/wiki/Lambda_calculus

Because Turing Machines are so simple, writing an interesting program in Turing-Machine code is very complicated. It would take a *long* time to write Windows in Turing-Machine code. But if you don’t want to *write* a program, but rather *prove* things about programs, then Turing Machines are you beast. The fact that Turing-computability is the same as computability by a normal computer (and all possible computers, as far as we know) means that all results you obtain about Turing-computability hold for computability more generally. The fact that Turing machines are so simple means that it is much easier to prove things about computability by considering Turing Machines than by considering normal computers.

#Numbering Turing Machines [section_numbering]

We know that there are countably many Turing Machines. So we know that there is a way of assigning a different natural number to each Turing Machine.

We can, in fact, *describe* a particular way of assigning numbers to Turing Machines. What’s more, we can describe a way of doing this in enough detail that two things are true:
	a) were you to give me the specifications of a random Turing Machine, I could tell you which number is assigned to it, and, more importantly,
	b) if you give me a number, I can tell you if that number corresponds to a Turing Machine, and, if so, which.
	In short, we can describe a way of *coding* Turing Machines as natural numbers. Our code will allow you to recover the Turing Machine from a given natural number.
	This is an important fact that we will use in proofs to come. Let’s pick a particular coding scheme and look at the details, to get clear on how it can be done.

##Two-Symbol Machines [section_two-symbol]

Before describing the coding scheme in detail, it will be useful to make a simplifying assumption. We will focus our attention on ‘two-symbol’ Turing Machines, that is, on Turing Machines in which the tape is only allowed to contain ones and blanks. 

As it turns out, this restriction is no restriction at all, because every function that can be computed on a many-symbol Turing Machine can also be computed on a two-symbol Turing Machine. There will be a question about this on your problem set.

##Our Scheme [section_scheme]
 
 Recall that each command line in a Turing Machine is a five-tuple of symbols:

<current state> <current symbol> <new symbol> <direction> <new state>

By making a few simple substitutions we can code such a sequence as a sequence of five natural numbers:

 * <current state> and <new state> are natural numbers already, so we can leave them as they are.
 * <current symbol> and  <new symbol> are either “1” or “_“. (Remember that we are working with two-symbol Turing machines.) So all we need to do is substitute “0” for “_“.
 * <direction> is “l”, “r” or “*”, which we can substitute ‘0’, ‘1’ and ‘2’ for, respectively. 
So, for example, the command line:

0 _ 1 r 1

becomes the following sequence of natural numbers:

0 0 1 1 1

Now consider a Turing Machine’s entire program. We can code it as a sequence of natural numbers in two steps: (1) code each line of the program as a sequence of five numbers, and (2) write out each of the resulting five-number sequences, one after the other. So, for instance, the program:

0 _ 1 * 0

0 1 1 r 1

gets coded as:

0 0 1 2 0 0 1 1 1 1

What we have succeeded in doing so far is represent a Turing Machine as a sequence of natural numbers, but remember that our objective is to code a Turing Machine as a single natural number. So what we need now is a method for coding each sequence of natural numbers as a unique natural number.

There are many ways of doing this. But the method we will consider here is due to Kurt Gödel, about whom you’ll be hearing more soon. It works like this:

Code the sequence \(n_1, n_2, \ldots, n_k\) as the number: \[p_{1}^{n_{1}+1}\times p_2^{n_2+1}\times\ldots\times p_k^{n_k+1}\] where \(p_1\ldots p_k\) are the first \(k\) prime numbers.

For instance, the sequence 0 0 1 1 1 gets coded as: \[2^{0+1}\times 3^{0+1}\times 5^{1+1}\times 7^{1+1}\times 11^{1+1} = 2\times 3\times 25\times 49\times 121 = 889,350\]

To verify that this gives us what we want, imagine that I have a particular two-state Turing Machine, and that I want to use a natural number to tell you which one it is. I proceed in two stages. First I represent the Turing Machine as a sequence of natural numbers. I then use Gödel’s method to code the sequence as a single natural number, and send you that number.

When you receive the number, will you be able to figure out which Turing Machine I started with? Yes! For recall the Fundamental Theorem of Arithmetic, which states that every positive integer greater than 2 has a unique decomposition into primes. This means if you factor the number I gave you into primes and look at the exponents, you’ll be able to recover the sequence of numbers I used in my coding. And once you have the sequence, you’ll have no trouble recovering the Turing Machine I started with. 

So we’re done: we’ve found a way to code Turing Machines as natural numbers.

#Non-Computable Functions [section_non-computable]

We know there are non-computable functions. Can we describe any, in any detail? Surprisingly, the answer is yes! Obviously, we can’t describe them in *complete* details — we can’t describe such a function in a way that would enable to you to compute the output for every input. But we can describe the function in enough detail for you to get the idea. In the section we’ll describe two important non-computable functions.

##The Halting Function [section_halting]

Suppose we give a Turing Machine a certain input, and we press “start”, the Turing Machine sets to work, computing. It goes on for a while, calculating, calculating… After a while, we start to wonder: is it ever going to halt? Has out Turing Machine got stuck in some loop, or something? Or, if we keep waiting, will we eventually get the answer we want? That’s an important question, when it comes to using Turing Machines, or any other computer, for that matter. We would like to know whether our computers, given their input, are going to halt, because a computer that doesn’t halt is useless.

The **Halting Function**, \(H(n,m)\), is a function from pairs of natural numbers to natural numbers, and is defined as follows:

\(H(n,m)=1\) if the \(n\)th Turning Machine halts when given input \(m\);

\(H(n,m)=0\) otherwise.

(Here we must assume a particular scheme for numbering Turing Machines. To fix ideas, I shall assume the scheme we characterised in an earlier subsection.)

Consider, for example, the Turing Machine whose program is the following:

0 _ _ r 0

This is the \(16170^\text{th}\) Turing Machine according to our scheme, because: \[2^{0+1}\times 3^{0+1}\times 5^{0+1}\times 7^{1+1}\times 11^{0+1} = 2\times 3\times 5\times 49\times 11 = 16170\]

When given input 0 (i.e. the empty string), this machine will start moving to the right and never stop. So the the \(16170\)th Turing Machine does not halt on input 0. So \(H(16170,0)=0\).

Is \(H(n,m)\) computable? That is: is there a Turing Machine we can use to check if any given Turing Machine, on a given input, will halt or not? Tragically, the answer is “no”.

###The Proof [section_proofhalting]

The proof is by *reductio*. We will assume that \(H(n,m)\) is Turing-computable, and prove a contradiction.

A little more specifically, we will proceed as follows. We will show that if \(H(n,m)\) were Turing-computable, then its program could be used to build an  Evil Turing Machine which halts if and only if it does‘t. But no Turing Machine can halt if and only if it doesn‘t. So the Evil Machine can‘t really exist. So \(H(n,m)\) can‘t really be Turing-computable.

Let’s get to work!

Let us assume that Turing Machine \(M^H\) computes the Halting Function. Consider \(M^H\)’s programme — i.e., the finite sequence of command lines. We can use those command lines as a subroutine in other Turing Machine programs. We will use that observation to describe the ‘Evil’ Turing Machine,  \(M^E\).

\(M^E\) works like this. When given input \(k\), it proceeds as follows:

1. It takes input \(k\) and outputs \(k\) “1”s, followed by a blank, followed by \(k\) “1”s.
2. It runs the \(M^H\) program, with \(\langle k, k\rangle\) as input — that is, it determines whether the \(k^\text{th}\) Turing Machine halts when given input \(k\).
3. If it gets the answer “yes” — i.e., if its tape is left with a “1” after performing the subroutine — it goes into an infinite loop.
4. Otherwise, it halts.

It isn’t too hard to show that a Turing Machine can perform steps 1, 3 and 4. Exercise for the reader: show it!

Okay, so that’s the Evil Machine. Now, let \(e\) be the code-number that our coding scheme assigns to \(M^E\), and consider the question of whether \(M^E\) halts given input \(e\). 

Suppose, first, that \(M^E\) does halt on input \(e\). This means that \(H(e,e)=1\). So the construction of \(M^E\) guarantees that when \(M^E\) is given \(e\) as input, it’ll go on an infinite loop (and therefore never halt).
Now suppose that \(M^E\) doesn’t halt. This means that \(H(e,e)=0\) . So the construction of \(M^E\) guarantees that when \(M^E\) is given \(e\) as input, it’ll halt.

So \(M^E\) halts if and only if it doesn’t halt. Contradiction!

So, \(M^E\) can’t really exist. So \(M^H\) can’t really exist. So the Halting Function isn’t Turing-computable.

##The Busy-Beaver Function [section_busy]

Let me define the **productivity** of a Turing Machine as follows:
	* If, given an empty input, a Turing Machine halts with output \(k\), we shall say that the machine’s productivity is \(k\).
	* If, given an empty input, the Turing Machine does anything other than halt with output \(k\), we shall say that its productivity is zero.

The **Busy Beaver Function**, \(BB(n)\), is a function from natural numbers to natural numbers defined as follows:

>\(BB(n)\) is the productivity of the most productive Turing Machine with \(n\) states or fewer.

It is known that \(BB(1)=1\), \(BB(2)=4\), \(BB(3)=6\) and \(BB(4)=13\), but for \(n\ge 5\), the exact value of \(BB(n)\) is not known (although certain kinds of bounds are known to exist).  

The Busy Beaver Function, like the Halting Function, is not Turing-computable.

###The Proof [section_proofbusy]

As in the case of the Halting Function, we’ll show that the Busy Beaver Function is not Turing-Computable by *reductio*. We’ll assume that there is a Turing Machine \(M^{BB}\) that computes the Busy Beaver function, and show that that would allow us to construct an evil Turing Machine \(M^E\). We will then use the evil Turing Machine to prove a contradiction.

Here is how the evil Turing Machine works, when given an empty tape as input:

* [Step 1]
For a particular number \(k\) (to be specified below), \(M^E\) starts by printing a sequence of \(k\) “1”s on the tape, and then brings the reader to the beginning of the resulting sequence. 

* [Step 2]
\(M^E\) duplicates the initial sequence of \(k\) “1”s, and brings the reader to the beginning of the resulting sequence of \(2k\) ones. 

* [Step 3]
\(M^E\) applies \(M^{BB}\) as a subroutine, which results in a sequence of \(BB(2k)\) “1”s. 

[Step 4]
\(M^E\) adds an additional “1” to the sequence of \(BB(2k)\) “1”s, returns to the beginning of the sequence, and halts, yielding \(BB(2k)+1\) as output. 

The way we’re going to prove that our evil machine, \(M^E\) leads to contradiction is by using it to show that (when \(k\) is large enough) both of the following evil inequalities must be true, even though they’re mutually inconsistent:

[E1] \(BB(s^E)\ge BB(2k)+1\)

[E2] \(BB(s^E)<BB(2k)+1\)

where \(s^E\) is the number of states in \(M^E\).



PROOF OF [E1]

The construction of  \(M^E\) guarantees that it delivers an output of \(BB(2k)+1\), given an empty input. So we know that the productivity of \(M^E\) is \(BB(2k)+1\). 

By definition, \(BB(s^E)\) is the productivity of the most productive Turing Machine with \(s^E\) states or less. But \(M^E\) has \(s^E\) states or less, and its productivity is \(BB(2k)+1\). So it must be the case that \(BB(s^E)\ge BB(2k)+1\) (which is [E1]).



PROOF OF [E2]

We will now show that, when \(k\) is a large enough number, [E2] must be true.

First, some notation:

Let \(a_k\) be the number of states required by [Step 1].

Since it is possible to build a Turing Machine that prints a sequence of \(k\) “1”s using no more than \(k\) states, and since it is possible to bring the reader to the beginning of the sequence using one additional state, we may assume that \(a_k\le k+1\).]

Let \(b\) be the number of states required by [Step 2] 

Since a fixed number of states can be used to duplicate sequences of “1”s of any length, (you will prove this in your problem set) \(b\) can be assumed to be a constant independent of \(k\).

Let \(c\) be the number of states required by [Step 3] 

\(c\) is simply the number of states in \(M^{BB}\), and is therefore a constant, independent of \(k\).

Let \(d\) be the number of states required by [Step 4] 

Since a fixed number of states can be used to add an extra ‘1’ at the end of a sequence of any length, \(d\) can be assumed to be a constant independent of \(k\).
This allows us to count the number of states that our evil machine requires: \[s^E=a_k+b+c+d \le (k+1)+b+c+d\]

where \(b\), \(c\) and \(d\) are independent of \(k\).

So as long as we pick \(k\) big enough for it to be the case that \(1+b+c+d \le k\), we will get the result that \(s^E\le 2k\). But this means that \(BB(s^E)\le BB(2k)\), and therefore that \(BB(s^E) < BB(2k)+1\) (which is [E2]).

SUMMARY

By looking at \(M^E\)’s output we showed that [E1] must be true. But by counting the steps in \(M^E\) we came to the contrary conclusion that [E2] must be true. So \(M^E\) cannot exist. So \(M^{BB}\) cannot exist. 

A similar construction can be used to show that \(BB(n)\) grows faster than any Turing computable function. (That is, if \(f(n)\) is computable, then there is an \(m\) such that, for every \(n>m\), \(BB(n)>f(n)\).) So \(BB(n)\) grows incredibly quickly! 

#The Universal Turing Machine [section_universal]

Consider the **Universal Computation Function** \(U(m,n)\). \(U(m,n)=f_m(n)\), when \(f_m(n)\) exists, where \(f_m\) is the function computed by Turing Machine \(m\). So \(U(m,n)\) outputs what Turing Machine \(m\) outputs, when given input \(n\), if it in fact outputs anything. Otherwise, if Turing Machine \(m\) given input \(n\) doesn’t halt, then \(U(m,n\) is undefined.

Is the Universal Computation Function Turing-computable? That is, is there a Turing Machine that outputs \(U(m,n)\), when it exists, and doesn’t halt otherwise? The answer is “yes”! Sometimes, the logic gods are kind.

The fact that the Universal Computation Function is computable is the reason why the software industry exists. Every computer that can run indefinite number of programs, like your laptop, is computing something like the Universal Computation Function.

The Turing Machine that calculates the Universal Computation Function is called the **Universal Turing Machine**; I’ll use “\(M^U\)” to denote it.  How does it work?

The basic idea is very simple: when  is given \(m\) and \(n\) as input it uses its tape to create a *simulation* of the behavior of Turing Machine \(m\) on input \(n\).

In slightly more detail: 

First, our machine uses \(m\) to print out a version of Turing Machine \(m\)’s program on the tape.

To the right of the program, our machine places a couple of markers. The space between the markers will serve as its “state memory”, with the number of “1”s between the markers representing the state of the simulated machine. (We start out by putting nothing between the two markers to represent the fact that our simulated machine starts out in state 0.)

To the right of the state memory, our machine prints out a sequence of \(n\) “1”s, preceded by a special symbol. The sequence of “1”s represents the fact that the simulated machine starts out with input \(n\), and the special symbol represents the position of the reader of the simulated machine on its tape. (The special symbol starts out at the beginning of the sequence of \(n\) ones to represent the fact that our simulated machine starts out with its reader at the beginning of its input.)

The machine then gets to work on the simulation. At each stage of the simulation, it determines whether the “reader” of the simulated machine is on a “1” or a “0”, and then uses the “state memory” to find the command from the simulated program that corresponds to that symbol and that state number. The machine then carries out the action determined by that command, and repeats.

#P=NP? [section_p=np?]

One of the most important open questions in mathematics today is the question of whether a certain class of Turing-computable functions — the P functions — are the same as another class of functions — the NP functions.

This problem is, in fact, one of the Clay Mathematics Institute’s “[Millennium Problems][millennium] ”. Those are a set of seven problems deemed to be especially important by the mathematics community; if you solve one of them, you can claim a million dollar prize!

The list was compiled in 2000; thus far, only one of the problems has been solved: Grigoriy Perelman, a Russian mathematician, proved the Poincaré conjecture in 2010, and refused the prize money! P=NP remains unsolved.

[millennium]:http://www.claymath.org/millennium-problems

##Tractability [section_tractability]

Some functions are computable, and some are not. That is somewhat sad news; it would be nice to be able to compute everything. But it’s not *so* sad, because it turns out that most things you can think to try to compute are, in fact, computable. Nobody has ever had to abandon writing a software update for Windows because it turned out that they were trying to do something non-computable.

But computing something takes *resources*. It takes time — a Turing Machine will take a certain number of steps before it halts, when it does. And it takes space — a Turing Machine will use a certain amount of tape, in the process of calculating. And the amount of resources it takes to compute something varies a lot, among computable functions. Sometimes it is not practical to compute a computable function, because it takes too much time or space to do it. 

Something that *does* happen, all the time, is that you’d like to compute something, but the only way you know how to do it takes much, much too long. For example, it would be nice to be able to factorise really large numbers — if we could do that, we could break the most common form of encryption used today, which would be cool. But a number doesn’t have to be too large before it gets very computationally expensive to factorise it. The stars would have grown cold before our best algorithms, running on the most powerful computers, factorised a fifteen digit number.

So, for practical purposes, it is helpful to be able to distinguish between those computable functions that we can compute in a reasonable amount of time, and those that we can’t. That is what the field of computational complexity is all about.

##Measuring Complexity [section_measuring]

We want to measure the complexity of a function in terms of the resources it takes to compute it. Now, we could, for instance, just count the number of steps it takes a Turing Machine to compute the function. The problem with that is that exactly how many steps it takes a Turing Machine to compute something depends very delicately on its architecture. If you change the Turing Machine a little bit, so that you use two tapes rather than one, or use a two-dimensional grid, or use RAM, that has a huge effect on how many steps it takes to compute the relevant function.

What we want is a measure of complexity that is robust across many different ways of computing things. We want to get at a property of the function, rather than a property of the thing computing the function. It turns out that a good way to do that is to talk about how the resources required to compute a function *scales* with the size of the input. 

##Exponential Time [section_exponential]

For example, for a certain class of functions, the time it takes to calculate the function grows *exponentially* with the size of the input. Here is an example of such a function. Take a map with a certain number of regions. You want to know how to colour it with four colours in such a way that no two adjacent regions get the same colour. This isn’t exactly right, but roughly speaking: for every extra region you add to the map, the time it takes to calculate this function, using the best mesthod we know of, doubles.

Problems that take exponential time to solve often involve some large, combinatorial space of possibilities that you need to search through to find your answer. Minesweeper or Candy Crush or Sudoku all require exponential time algorithms to solve.

It is bad news if the best way you can find to calculate a function takes exponential time. It means that the input does not have to be too large before you have to wait completely impractical amounts of time for your machine to give you an answer.

##Polynomial Time [section_polynomial]

On the other hand, for some functions, we can find ways of calculating them grows as a polynomial function of the size of the input. For example, if you give the best pallindrome-verifying Turing Machine a input \(n\) symbols long, and ask it to determine whether the input is a pallindrome or not, the time it will take to do that, in the worse case, is roughly \(n^2\).

If your problem can be solved with a polynomial time function, that is good news! That generally means that you can get an answer in a reasonable amount of time, even for large inputs. So it is important for us to determine 

The set of polynomial-time functions is called “P”.

##P vs. NP [section_pvsnp]

NP is the set of functions such that, if I tell you what the output is, for a given input, you can verify that I’m right using a polynomial time function.

For example: if I tell you that the prime factors of some large number are \(x\) and \(y\), then you can easily verify this fact by multiplying \(x\) and \(y\) together, which can be done with a polynomial time function. (You can also check that those numbers are indeed prime in polynomial time.)

All functions in P are in NP, obviously — you can verify the answer in polynomial time just by running the function and getting answer, when the function is in P. But are all NP functions P functions? That’s the question.

Almost all computer scientists and mathematicians think the answer is “no”. But we don’t have a proof!